%% LyX 2.1.2.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt,english]{beamer}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 % this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{listings}
\usetheme{Warsaw}
% or ...
%\usetheme{Antibes}	% tree outline, neat
%\usetheme{JuanLesPins}	% like Antibes, with shading
%\usetheme{Bergen}	% outline on side
%\usetheme{Luebeck}	% like Warsaw, square sides
%\usetheme{Berkeley}	% interesting left bar outline
%\usetheme{Madrid}	% clean, nice.  7/12 page numbers
%\usetheme{Berlin}	% dots show slide number
%\usetheme{Malmoe}	% OK, plain, unshaded
%\usetheme{Boadilla}	% nice, white bg, no top bar
%\usetheme{Marburg}	% nice, outline on right
%\usetheme{boxes}	% ???
%\usetheme{Montpellier}	% tree outline on top, plainish white
%\usetheme{Copenhagen}	% like Warsaw
%\usetheme{PaloAlto}	% looks good
%\usetheme{Darmstadt}	% like Warsaw with circle outline
%\usetheme{Pittsburgh}
%\usetheme{default}
%\usetheme{Rochester}	% like boxy, unshaded warsaw
%\usetheme{Dresden}	% circle outline on top
%\usetheme{Singapore}	% purple gradient top
%\usetheme{Frankfurt}	% like Warsaw with circle outline on top
%\usetheme{Szeged}
%\usetheme{Goettingen}	% light purple right bar outline
%\usetheme{Warsaw}
%\usetheme{Hannover}	% like Goett with bar on left
%\usetheme{compatibility}
%\usetheme{Ilmenau}

\setbeamercovered{transparent}
% or whatever (possibly just delete it)

%\usecolortheme{seahorse}
%\usecolortheme{rose}

% seems to fix typewriter font in outline header:
\usepackage{ae,aecompl}

\makeatother

\usepackage{babel}

\title{Unlabeled Data: Now It Helps, Now It Doesn't}
\subtitle{A. Singh, R. D. Nowak, and X. Zhu. In NIPS, 2008. 1}
\author{Mark Andrew Ward and Max Kuang}
\institute{Courant Institute, NYU}
\begin{document}


\pgfdeclareimage[height=0.5cm]{institution-logo}{institution-logo-filenameO}

\logo{\pgfuseimage{institution-logo}}

% RPD:  can't get this to work on any template.  not present in Warsaw any way, it seems

% hmm, problem seems to be that it isn't copied to the tmp dir, probably becuase it doesn't have the

% filename extension (which is tacked on by pgf it seems)

%\AtBeginSection[]{
\begin{frame}
\titlepage
\end{frame}
\AtBeginSection[]{

  \frame<beamer>{ 

    \frametitle{Outline}   

    \tableofcontents[currentsection,currentsubsection] 
%    \tableofcontents[currentsection] 

  }

}
%\beamerdefaultoverlayspecification{<+->}
\begin{frame}
     \frametitle{Outline}   
    \tableofcontents[currentsection,currentsubsection,sectionstyle = show,subsectionstyle = show] 
\end{frame}

\section{Introduction}
\subsection{Conflicting Views in Semi-supervised Learning}
\begin{frame}
\frametitle{\insertsubsection}

\begin{itemize}
\item  Given $n$ iid labeled sample $\{(x_i, y_i)\}_{i = 1,\dots, n}$ and $m$ iid unlabeled sample $\{x'_i\}_{i = 1,\dots, m}$,
can we do better than supervised learning from merely $n$ labeled points $\{(x_i, y_i)\}_{i = 1,\dots, n}$.
 \item Not always better: Only when there exists a \textbf{link} between the \textbf{marginal data distribution $P(x)$} and the \textbf{target function to be learned $y = f(x)$}.
\item \textbf{Links}: \textbf{cluster assumption} and \textbf{manifold assumption}.
 \item Does unlabeled data help in error convergence rate under different assumptions?
\end{itemize}
\begin{center}
\begin{tabular}{l||ll}
 & SSL helps & SSL does not help\\ \hline\hline
Cluster assumption & Castelli and Cover\cite{Cover1995,Cover1996} & Rigollet\cite{Rigollet2007}\\
Manifold assumption & Lafferey and Wasserman\cite{Lafferty2008} & Niyogi\cite{Niyogi2008}
\end{tabular}
\end{center}
\end{frame}
\begin{frame}
\frametitle{\insertsubsection}
\begin{itemize}
\item
This work focuses on learning under the \textbf{cluster assumption} and provides \textbf{finite sample bounds} to identify situations in which unlabeled data will help to improve learning.
\end{itemize}
\begin{figure}[h]
\centering
 \includegraphics[width = 4in]{1.png}
\end{figure}
\end{frame}

\subsection{The Cluster Assumption}
\begin{frame}
 \frametitle{\insertsubsection: Marginal Distributions}
\begin{itemize}
\item The marginal distribution $p(x) = \sum_{k=1}^K a_k p_k(x)$ is the mixture of a finite, but unknown, number of component densities $\{p_k\}_{k=1}^K$.
\item Restrictions on $p_k$:
\begin{enumerate}
 \item $p_k$ is supported on a compact connected set $C_k\in\mathcal{X}$ with Lipschitz boundaries. Specifically:
\begin{equation}
\begin{array}{ll}
 C_k = \{&x\equiv (x_1,x_2,\dots,x_d)\in\mathcal{X}: 
\\&g_k^{(1)}(x_1,x_2,\dots,x_{d-1}) \leq x_d\leq  g_k^{(2)}(x_1,x_2,\dots,x_{d-1})\}
\end{array}
\end{equation}

\item $p_k$ is bounded from above and below, $0<b\leq p_k\leq B$.
\item $p_k$ is Holder-$\alpha$ smooth on $C_k$ with Holder constant $K_1$.
\end{enumerate}
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Dicision Sets}
\begin{itemize}
 \item Let $\mathcal{D}$ denote the collection of all non-empty sets obtained as intersections of $\{C_k\}_{k=1}^K$.
 \item \textbf{Cluster assumption: the target function $y = f(x)$ to be learnt is smooth on each set $D\in\mathcal{D}$}.
\begin{figure}[h]
\centering
 \includegraphics[width = 2.5in]{4.png}
\end{figure}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{\insertsubsection: Margin $\gamma$}
\begin{itemize}
 \item The margin $\gamma$ of a distribution is defined to be the minimal width of a decision set.
\item The margin $\gamma$ is assigned a positive sign if there is no overlap between components,otherwise it is assigned a negative sign.
\begin{figure}[h]
\centering
 \includegraphics[width = 4in]{2.png}
\end{figure}
\end{itemize}
\end{frame}


\section{Finite Sample Analysis of Semi-supervised Learning}
\subsection{Summary}
\begin{frame}
 \frametitle{\insertsubsection}
\begin{itemize}
 \item Under cluster assumption, we are trying to figure out for what $(n,m,\gamma)$(and possibly other constraints) SSL surpasses SL
for all general learners.
\begin{center}
  \large    SSL > SL\\
$\Uparrow$\\
SSL Learner $\approx$ Clairvoyant Learner > General SL Learner
$\Uparrow$(definition)\\
\textit{Supervised learners with perfect knowledge of the decision sets $\mathcal{D}$}
 \end{center}
\end{itemize}
\end{frame}
\subsection{Learning the Decision Sets}
\begin{frame}
 \frametitle{\insertsubsection: \\SSL Learner $\approx$ Clairvoyant Learner}
\begin{itemize}
 \item Decision sets are learnable using unlabeled data: marginal density $p$ is smooth within each decision set but exhibits jumps at the decision set boundaries.
\item Main learning procedure:
\begin{enumerate}
 \item Marginal Density Estimation: From unlabeled sample $\{x_i\}_{i = 1,\dots, m}$ to density estimator $\hat{p}(x)$
\item Decision Set Estimation: From  $\hat{p}(x)$ to decision set estimator $\hat{\mathcal{D}}$
\end{enumerate}
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Marginal Density Estimation}
\begin{itemize}
 \item Sup-norm kernel density estimator\cite{Koro1999}: Consider a uniform grid over the feature space $\mathcal{X} = [0,1]^d$ with spacing $2h_m$, where $h_m = \kappa_0 ((\log m)^2/m)^{1/d}$.
\item Make a histogram-style density estimation on the grid using kernel density estimation: 
\begin{equation}
 \hat{p}(x) = \frac{1}{mh_m^d}\sum_{i = 1}^m G(H^{-1}_m(X_i - \bar{x}))
\end{equation}
Where $\bar{x}$ is the closest point to $x$ on the grid, $G$ is the kernel and $H_m = h_mI$
\begin{figure}[h]
\centering
 \includegraphics[width = 3in]{5.png}
\end{figure}
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Decision Set Estimation}
\begin{itemize}
 \item Locating the jumps in $\hat{p}(x)$: \textbf{p-connectivity} of data points.
\item Two point $x_1,x_2\in \mathcal{X}$ are said to be \textbf{connected},
if there exists a sequence of points $x_1 = z_1 , z_2 ,\dots, z_l= x_2$ such that $z_2 , . . . , z_{l-1} \in U$,$\|z_j - z_{j+1}\leq 2\sqrt{d}h_m\|$.
\item Two point $x_1,x_2\in \mathcal{X}$ are said to be \textbf{p-connected}, if in addition to being \textbf{connected}, 
we have $|\hat{p}(z_i) - \hat{p}(z_j)| \leq (\log m)^{-1/3}$ for all $z_i, z_j$ satisfying $\|z_j - z_{j+1}\|\leq h_m\log m$.
\item All points that are pairwise \textbf{p-connected} specify an empirical decision set $\hat{D}$ and we derived $\hat{\mathcal{D}}$.
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Guarantee}
\begin{figure}[h]
\centering
 \includegraphics[width = 4in]{6.png}
\end{figure}
\begin{itemize}
 \item Exclude decision boundaries, p-connectivity is equivalent to decision set with high probability.
\item Margin $\gamma$ plays an important role: theorem works only when $|\gamma|$ is in the order of the spacing $h_m$.
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Proof}
 \begin{enumerate}
  \item Uniform bound for density estimation: Given certain assumptions on Kernels, we have
  with probability at least $1 - \frac{1}{m}$:
  \begin{equation}
   \sup_{x\in supp(p)\backslash\mathcal{R_B}}|p(x) - \hat{p}(x)| < \bigg|h_m^{\min(1,\alpha_1)}+ \sqrt{\frac{\log m}{m _m^d}}\bigg|
  \end{equation}

  \item Conectivity: For all $x\in supp(p)\backslash\mathcal{R_B}$, with probability $1-\frac{1}{m}$, there exsits an unlabeled sample $X_i$ that
  $\|X_i - x\|< \sqrt{d}h_m$

  \item Bound $|\hat{p}(x) - \hat{p}(x')|$ using nearby unlabeled points $z,z'$:
$|\hat{p}(x) - \hat{p}(z)|$,$|\hat{p}(z) - \hat{p}(z')|$, $|\hat{p}(z) - p(z)|$ and $|p(z) - p(z')|$
 \end{enumerate}


\end{frame}

\subsection{SSL Performance Analysis}
\begin{frame}
 \frametitle{\insertsubsection}
 \begin{itemize}
  \item Let $\mathcal{R}(f)$ denote the risk of interest for a given target function $f$ and excess risk $\mathcal{E}(f) = \mathcal{R}(f) - \mathcal{R^*}$,
  where $\mathcal{R^*}$ is the infimum risk over all possible learners.
  \item SSL learner$\approx$ clairvoyant learner:
 \end{itemize}

 \begin{figure}[h]
\centering
 \includegraphics[width = 4.3in]{7.png}
\end{figure}
\end{frame}
\begin{frame}
 \frametitle{\insertsubsection: Proof and Remarks}
 \begin{itemize}
  \item To prove the theorem, one only need to use the fact that $\hat{\mathcal{D}}$ is very close to $\mathcal{D}$
  in a probability sense. Using condition probability, $\E \mathcal{E}(\hat{f}_{m,n})$ is close to $\E \mathcal{E}(\hat{f}_{D,n})$.
  \item Conditions to make SSL Learner be close to clairvoyant learner are:
  \begin{enumerate}
   \item The margin $\gamma$ is large enough: $|\gamma| > C_0(m/(\log m)^2)^{-1/d} $
   \item The error term is smaller than $\varepsilon_2(n)$: $(n/\varepsilon_2(n))^d = O(m/(\log m)^2)$
  \end{enumerate}
  \item If the clairvoyant learner outperforms general SL learners: 
  \begin{equation}
   \inf_{f_n}\sup_{P_{XY}}\E[\mathcal{E}(f_n)] \geq \varepsilon_1(n) > \varepsilon_2(n)
  \end{equation}
  We have that there exists a SSL Learner that outperforms general SL learners.
 \end{itemize}

\end{frame}

\section{Density-adaptive Regression}
\subsection{Optimal Decision Rule}
\begin{frame}
   \frametitle{\insertsubsection: Definition}
  \begin{itemize}
	\item $Y$ continuous and bounded random variable
	\item $f^*(x)=\E[Y|X=X]$, under the squared error loss
	\item Let $\E_k$ denote expectation with respect to $p_k(Y|X=x)$ and define $f_k(x) = \E_k[Y|X=x]$ then 
	\begin{equation}
	f^* (x) = \sum_{k=1}^{K} \frac{\sum_{j=1}^{K} a_k p_k(x)} {a_j p_j(x)} f_k(x)
	\end{equation}
	\item Assumptions: 
	\begin{enumerate}
	\item $f_k$ is uniformly bounded, $|f_k| \leq M$
	\item $f_k$ is Holder-$\alpha$ smooth on $C_k$
	\end{enumerate}
   \end{itemize} 
\end{frame}

\subsection{SSL Algorithm}
\begin{frame}
   \frametitle{\insertsubsection}
   \begin{itemize}
     \item Since $f^*$ is smooth on each $D \in \mathcal{D}$, perform local polynomial fits within each empirical decision set, using labeled training data that are p-connected
     \item Use spatially adaptive estimator, optimal for piecewise-smooth functions
     \item Guarantee SSL still achieves an error bound that is no worse than lower bound for SL when components are indiscernible even with unlabeled data.
   \end{itemize}
\end{frame}
\begin{frame}
   \frametitle{\insertsubsection}
   \begin{itemize}
   \item Semi-supervised learner: 
   \begin{align}
     \hat{f}_{m,n,x}(\cdot) &= \argmin_{f^{'} \in \Gamma} \sum_{i=1}^{n} (Y_i - f'(X_i) )^2 \mathbf{1}_{x \overset{p}\longleftrightarrow X_i} + \text{pen}(f') \\
     \hat{f}_{m,n}(x) &\equiv  \hat{f}_{m,n,x}(\cdot) \notag
   \end{align}
   \item $\Gamma$: collection of piecewise polynomials, defined over a recursive dyadic partitioning of the domain $\mathcal{X} = [0,1]^d$
   \item pen($f'$) $\propto$ $log(\sum_{i=1}^{n}\mathbf{1}_{x \overset{p}\longleftrightarrow X_i}) \cdot \text{\#} f'$, where \#$f'$ is the number cells over which $f'$ is defined
   \end{itemize}
\end{frame}

\subsection{Error Bounds}
\begin{frame}
   \frametitle{\insertsubsection:  Overview}
   \begin{itemize}
   \item For piecewise Holder-$\alpha$ smooth functions, finite sample error bound of max$(n^{-2\alpha / (2\alpha+d)}, n^{-1/d})$
   \item Assume $m \gg n^{2d}$ so that  $\sup_{P_{XY}}\E[\mathcal{E}(\hat{f}_{m,n})]$ scales as $\varepsilon_2(n)$
   \item Assume $d \geq 2\alpha / (2\alpha -1)$, since when $d < 2\alpha / (2\alpha -1)$ learning decision sets does not simplify supervised learning task
   \end{itemize}
   \begin{figure}[h]
      \centering
      \includegraphics[width = 4.0in]{8.png}
     \end{figure}
 \end{frame}
\begin{frame}
   \frametitle{\insertsubsection:  Overview}
    \begin{figure}[h]
      \centering
      \includegraphics[width = 4.0in]{8.png}
     \end{figure}
     \begin{itemize}
     \item $\gamma_0$ fixed constant, corresponds to considering a fixed collection of distributions whose complexity does not change with the amount of data
     \item Constants $C_0$ and $c_0$ characterize margin and only depend on fixed parameters of the class $P_{XY}(\gamma)$
     \end{itemize}
\end{frame}
\begin{frame}
\begin{itemize}
\item Based on theorem from Castro 2005. Let $n_D = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}_{x \in D}$ 
    \begin{figure}[h]
      \centering
      \includegraphics[width = 2.8in]{10.png}
     \end{figure}
 \item Decompose the error of the estimator in to three different cases
\end{itemize}
   \frametitle{\insertsubsection:  Proof}
    \begin{figure}[h]
      \centering
      \includegraphics[width = 4.0in]{9.png}
     \end{figure}
\end{frame}

\section{Concluding Remarks}
\begin{frame}
   \frametitle{\insertsubsection}
  \begin{itemize}
  \item Under the cluster assumption, there exist general situations which SSL can be significantly better than SL in terms of achieving smaller finite sample error bounds than any SL
  \item Likely that similar conclusion may be drawn under the manifold assumption where the curvature of the manifold will play a similar role to the margin under the cluster assumption
  \item Showed SSL simplifies learning when there is a link between the marginal and conditional distributions holds
  \item Interested in SSL whose performance does not deteriorate when the link or margin is not discernible using unlabeled data or does not hold
  \item Ensure SSL performance is no worse than what SL would achieve such as in Density-adaptive Regression 
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
  \frametitle<presentation>{Further Reading}    
  \begin{thebibliography}{10}    
\tiny
  \beamertemplatearticlebibitems
  \bibitem{Cover1995}
Castelli, V., Cover, T.M.
\newblock{\em On the exponential value of labeled samples}
\newblock Pattern Recognition Letters 16(1) (1995) 105-111

  \beamertemplatearticlebibitems
  \bibitem{Cover1996}
Castelli, V., Cover, T.M.
\newblock {\em The relative value of labeled and unlabeled samples in pattern recognition}
\newblock IEEE Transactions on Information Theory 42(6) (1996) 2102-2117
  \beamertemplatearticlebibitems
  \bibitem{Lafferty2008}
Lafferty, J., Wasserman, L.
\newblock {\em Statistical analysis of semi-supervised regression}
\newblock Advances in Neural Information Processing Systems 20, NIPS. (2008) 801-808
  \beamertemplatearticlebibitems
  \bibitem{Niyogi2008}
Niyogi, P.
\newblock {\em Manifold regularization and semi-supervised learning: Some theoretical analyses}
\newblock Technical Report TR-2008-01, Computer Science Department, University of Chicago.

  \beamertemplatearticlebibitems
  \bibitem{Rigollet2007}
Rigollet, P.
\newblock {\em Generalization error bounds in semi-supervised classification under the cluster assumption}
\newblock Journal of Machine Learning Research 8 (2007) 1369-1392

  \beamertemplatearticlebibitems
  \bibitem{Koro1999}
Korostelev, A., Nussbaum, M. 
\newblock {\em The asymptotic minimax constant for sup-norm loss in nonparametric
density estimation} 
\newblock Bernoulli 5(6) (1999) 1099-1118

  \beamertemplatearticlebibitems
  \bibitem{Castro 2005}
Castro, R., Willett, R., Nowak, R. 
\newblock {\em Faster rates in regression via active learning} 
\newblock Advances in Neural Information Processing Systems, NIPS. (2005) 179-186

  \end{thebibliography}
\end{frame}   
\end{document}


